# ğŸ§  Multi-Armed Bandit Hackathon â€“ Student Edition  
**Course:** Network Intelligence  
**Topic:** Reinforcement Learning Foundations â€” Bandit Algorithms  

---

## ğŸ¯ Objective

In this mini-project you will:
- Understand the concept of **exploration vs. exploitation** through a practical experiment.
- Implement your own **Multi-Armed Bandit (MAB)** algorithm.
- Test how well it performs on a simulated environment.
- Compete (friendly!) with your classmates in a final evaluation.

This is your **first Python project** for this course, so the goal is not just to win â€” itâ€™s to **learn by doing**.

---

## ğŸ§© What Is a Multi-Armed Bandit?

Imagine you enter a casino with 10 slot machines (the â€œarmsâ€).  
Each arm gives a reward when you pull it, but the average reward is **different and unknown**.  
Your mission is to find a strategy that maximizes your total reward:
- **Explore** â†’ try different arms to gather information.  
- **Exploit** â†’ focus on the arm that seems best so far.

Thatâ€™s exactly what youâ€™ll do in code!

---

## ğŸ“ Project Structure

After unzipping this folder, youâ€™ll see:

```
mab-hackathon-student/
â”‚
â”œâ”€â”€ README.md                   â† This document (read it carefully!)
â”‚
â”œâ”€â”€ submissions/
â”‚   â”œâ”€â”€ template_agent.py        â† Template file: COPY and EDIT this!
â”‚   â””â”€â”€ my_agent.py              â† Your version (you will create it)
â”‚
â””â”€â”€ student_sandbox/
    â”œâ”€â”€ simple_env.py            â† The environment (do NOT edit)
    â””â”€â”€ quick_test.py            â† Testing tool (you will use it to run and plot)
```

---

## ğŸ§° Required Software

You only need:
- **Python 3.10 or higher**
- **Pip (Python package manager)**

If you donâ€™t have them yet:
- [Download Python here](https://www.python.org/downloads/)  
- During installation, **tick â€œAdd Python to PATHâ€**

---

## âš™ï¸ Installing Dependencies

Open a terminal (or command prompt) inside the project folder and run:

```bash
pip install numpy matplotlib
```

Thatâ€™s all you need â€” these libraries handle math and plotting.

---

## ğŸ” Step 1 â€” Understand Each File

### ğŸ§  `simple_env.py`  
This is the **simulation environment** (you donâ€™t touch it).  
It mimics a casino with multiple slot machines (â€œarmsâ€).  
Each arm has a hidden average reward `q_star[a]`.  
When you call:

```python
reward, info = env.step(arm)
```

you get a **random reward** around that average.

ğŸ’¡ Think of it as a black box that returns rewards when you choose an arm.

---

### âš™ï¸ `template_agent.py`  
This is the **starting point** of your algorithm.

It defines:
- Your **team name** and **algorithm name**
- The **class structure** that your code must follow
- A **working example**: a simple *Îµ-greedy* agent.

You will copy this file and edit it.

---

### ğŸ“ˆ `quick_test.py`  
This file lets you **test and visualize** how your algorithm performs.  
You donâ€™t need to edit it, but youâ€™ll run it a lot.

It:
1. Loads your agent automatically.
2. Runs it for many episodes (games).
3. Plots the **average reward curve** so you can see if your algorithm improves over time.

---

## âœï¸ Step 2 â€” Create Your Own Agent

1. Go inside the folder `submissions/`
2. **Copy** the file `template_agent.py`
3. **Rename** the copy as:

```
my_agent.py
```

This is the **only file you will submit**.

---

## ğŸ§  Step 3 â€” Fill In Your Agentâ€™s Information

At the top of `my_agent.py`, change:

```python
TEAM_NAME = "TEAM_NAME_HERE"
ALGO_NAME = "MyEpsGreedy"
```

For example:
```python
TEAM_NAME = "Alice_Rossi"
ALGO_NAME = "DecayingEpsilonGreedy"
```

---

## ğŸ’» Step 4 â€” Understand the Class `BanditAgent`

Youâ€™ll see:

```python
class BanditAgent:
    def __init__(self, n_arms: int, seed: int, epsilon: float=0.1):
        ...
    def reset(self, seed: int | None = None):
        ...
    def select_arm(self, context=None) -> int:
        ...
    def update(self, arm: int, reward: float, info: dict | None = None):
        ...
```

Hereâ€™s what each function does:

| Function | Called by | You shouldâ€¦ |
|:----------|:-----------|:-------------|
| `__init__()` | When the agent is created | Set up variables, e.g. `epsilon`, `Q` values, counters |
| `reset()` | At the start of every episode | Reinitialize everything |
| `select_arm()` | Every step before the environment returns a reward | Choose which arm to play |
| `update()` | After you get a reward | Update your statistics (means, counters, etc.) |

âœ… **You are free to change the logic inside these methods.**  
âŒ **Donâ€™t change their names or arguments.**

---

## ğŸ”¬ Step 5 â€” Run and Visualize Your Algorithm

Open a terminal and run:

```bash
python student_sandbox/quick_test.py
```

If everything is working, youâ€™ll see a **plot** like this:

```
Average reward increasing over time â†’
Good! It means your agent is learning.
```

If you see errors:
- Check that your file is called `my_agent.py`
- Check that all function names are correct (`select_arm`, `update`, â€¦)

---

## ğŸ§® Step 6 â€” Experiment and Improve

Try to modify your code to implement different algorithms:
- Change the exploration rate `epsilon`
- Make `epsilon` **decrease over time** (`epsilon = 1 / t`)
- Implement **UCB (Upper Confidence Bound)** or **Thompson Sampling**

Each time, re-run:

```bash
python student_sandbox/quick_test.py
```

and compare your results!

---

## ğŸ Step 7 â€” Submit

When youâ€™re happy with your results:

1. Make sure your file runs correctly.
2. Submit only this file:

```
submissions/my_agent.py
```

No other files are needed.  
The professor will test it on hidden environments.

---

## ğŸ§© Understanding the API (Advanced Reference)

When the professor runs your agent, this happens under the hood:

```python
for t in range(steps):
    a = agent.select_arm()
    reward, info = env.step(a)
    agent.update(a, reward, info)
```

Thatâ€™s why:
- `select_arm()` must **return an integer** (the arm index).
- `update()` must **receive** the `arm` and the `reward`.

---

## ğŸ§  Tips & Good Practices

- Keep your variable names clear, e.g. `self.counts`, `self.Q`.
- Add comments to explain your idea.
- Start simple, then improve gradually.
- You can test multiple agents:  
  copy `my_agent.py` â†’ `my_agent_v2.py` â†’ `my_agent_v3.py` and switch which one runs in `quick_test.py`.

---

## ğŸ’¬ Need Help?

You can ask ChatGPT for help with coding *as long as* you follow the required structure.

Example prompts you can try:
- â€œExplain the difference between exploration and exploitation in MAB.â€
- â€œShow me pseudocode for UCB1 using incremental mean updates.â€
- â€œHow do I make epsilon decay over time in an epsilon-greedy algorithm?â€

---

## ğŸ§¾ Summary Table

| File | Purpose | Can I edit it? |
|:--|:--|:--:|
| `submissions/template_agent.py` | Template with working example | âœ… copy & edit |
| `submissions/my_agent.py` | Your implementation | âœ… |
| `student_sandbox/simple_env.py` | The environment | âŒ |
| `student_sandbox/quick_test.py` | Test and visualize | âš ï¸ only if you want to change plots |
| `README.md` | Instructions | âœ… (for notes) |

---

## ğŸ§¡ Closing Notes

This small project is designed to make you:
- write **your first working AI agent**,
- understand **reinforcement learning** intuitively,
- and gain confidence in experimenting with code.

Be creative, curious, and have fun â€” your algorithm will soon compete in the class leaderboard!
